{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = 12, 8\n",
    "\n",
    "from skimage.transform import resize\n",
    "import skimage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UWavy Navigation - Solution\n",
    "\n",
    "1. [Overview](#Overview)\n",
    "2. [Inertial Navigation](#Inertial-Navigation)\n",
    "3. [Image Recognition](#Image-Recognition)\n",
    "4. [Synthesis](#Synthesis)\n",
    "5. [Conclusion](#Conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inertial Navigation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating a \"True\" Path\n",
    "[TODO: Animation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wide_uturn(leg=10):\n",
    "    ones = np.ones((leg,1));\n",
    "    zeros = np.zeros((leg,1));\n",
    "    \n",
    "    leg1 = np.hstack((ones, zeros))\n",
    "    leg2 = np.hstack((zeros, ones))\n",
    "    leg3 = np.hstack((-ones, zeros))\n",
    "    return np.concatenate((leg1, leg2, leg3), axis=0)\n",
    "\n",
    "def s_turn(short_leg=5):\n",
    "    ones = np.ones((short_leg,1));\n",
    "    zeros = np.zeros((short_leg,1));\n",
    "    \n",
    "    leg1 = np.hstack((ones, zeros))\n",
    "    leg2 = np.hstack((zeros*2, -ones*2))\n",
    "    leg3 = np.hstack((ones, zeros))\n",
    "    return np.concatenate((leg1, leg2, leg3), axis=0)\n",
    "\n",
    "# Set out a path, starting at START_POS and following updates along DELTAS_TRUE\n",
    "start_pos = np.array([[1500,850]])\n",
    "deltas_true = (np.vstack((np.vstack([s_turn(1) for _ in range(3)]), wide_uturn(3))) * 25).astype(np.float64)\n",
    "path_true = np.cumsum(np.vstack((start_pos, deltas_true)), axis=0)\n",
    "\n",
    "## TODO Plot path_true\n",
    "ani = FlightAnimator()\n",
    "ani.add_path(path_true, label=\"True Path\")\n",
    "ani.start();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introducing Systematic and Random Error\n",
    "[TODO: Animation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate some input from the onboard sensors and camera\n",
    "deltas_inert = (deltas_true - np.array([3, -1])).astype(np.float64) # Add systematically biased error \n",
    "# TODO add random error\n",
    "ani = FlightAnimator()\n",
    "ani.add_path(path_sim, label=\"Simulated Path\")\n",
    "ani.start();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Recognition\n",
    "As can be seen abocve, inertial navigation on its own is not very impressive, but combining it with images taken from a drone's onboard camera can yield exceptional results. This section explains how the image recognition part of the software works by examining some of the underlying algorithms and walking through a simple demo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulating the image database\n",
    "\n",
    "In practive, a drone will use its onboard camera to take an image of the ground. This image must be matched with a reference image whose location is already known. A match indicates that the position of the drone is the same as the position of the reference image.\n",
    "\n",
    "We simulate this process using a single reference image. In practice, we could have many images, layered on top of each other and stiched together, but this most basic MVP serves as a compelling proof of concept. Our reference image—seen below—is a [map of New York City](https://en.wikipedia.org/wiki/File:Aster_newyorkcity_lrg.jpg) comprising all five boroughs with a resolution of 5,125 pixels per square kilometer. Future iterations will use multiple such images stiched together, but this should work fairly well for navigation about a single city such as New York. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "ref_img = np.array(Image.open('manhattan.jpg'))\n",
    "RADIUS = 180\n",
    "\n",
    "def take_picture(drone_position):\n",
    "    x_center = drone_position[0]\n",
    "    y_center = drone_position[1]\n",
    "    return ref_img[x_center - RADIUS : x_center + RADIUS, y_center - RADIUS : y_center + RADIUS]\n",
    "\n",
    "def plot_ref_img(ax=None, drone_pos=None, crop_height=1):\n",
    "    if ax is None:\n",
    "        plt.title(\"Reference Image\")\n",
    "        ax = plt\n",
    "    else:\n",
    "        ax.set_title(\"Reference Image\")\n",
    "    ax.imshow(ref_img.take(range(int(ref_img.shape[0] * crop_height)), axis=0), alpha = 1 if drone_pos is None else .75 );\n",
    "    ax.axis('off');\n",
    "    \n",
    "    if drone_pos is not None:\n",
    "        from matplotlib.patches import Rectangle;\n",
    "        N = 150;\n",
    "        pos = np.flip(drone_pos - RADIUS)\n",
    "        ax.add_patch(Rectangle(pos, RADIUS * 2, RADIUS * 2, color=\"dodgerblue\", fill=False, linewidth=3, label=\"Camera View\"));\n",
    "        plt.legend()\n",
    "        \n",
    "def plot_line_error(drone_position, ax):\n",
    "    # Plot a horizontal line representing the positions searched\n",
    "    ax.axhline(drone_position[0])\n",
    "    \n",
    "    # Take successive slices of REF_IMG along a vertical axis that runs through (x,y) and plot the error \n",
    "    ref_slice = take_picture(drone_position)\n",
    "    errs = []\n",
    "    rng = range(0, ref_img.shape[1], 5)\n",
    "    for i, x in enumerate(rng):\n",
    "        test_slice = take_picture((x + RADIUS, drone_position[0]))\n",
    "        err = np.sum(np.square(test_slice - ref_slice)) / (4*RADIUS*RADIUS)\n",
    "        errs.append(err)\n",
    "    errs = np.array(errs)\n",
    "    ax.twinx().plot(rng, errs, 'r', linewidth=3, label=\"Mean Squared Error\");\n",
    "    \n",
    "def plot_heat_map(drone_position, ax):\n",
    "    if ax is None:\n",
    "        plt.title(\"Error Heatmap\")\n",
    "        ax = plt\n",
    "    else:\n",
    "        ax.set_title(\"Error Heatmap\")\n",
    "    # Take a sample of size NxN centered at drone_position\n",
    "#     ref_slice = sample_square(ref_img, x, y, N//2)\n",
    "    ref_slice = take_picture(drone_position)\n",
    "\n",
    "#     ax[1].add_patch(Rectangle((x - N//2, y - N//2), N, N, color=\"blue\", fill=False, linewidth=3))\n",
    "\n",
    "    # Make a heatmap showing the differences between slices taken at each point on the map and the test slice\n",
    "    x_rng, y_rng = [range(RADIUS, ref_img.shape[i] - RADIUS, 25) for i in (0,1)]\n",
    "    errs = np.zeros((len(x_rng), len(y_rng)))\n",
    "    for (i,x) in enumerate(x_rng):\n",
    "        for (j,y) in enumerate(y_rng):\n",
    "#             test_img = sample_square(ref_img, x, y, RADIUS)\n",
    "            test_img = take_picture((x,y))\n",
    "            err = np.sum(np.square(test_img - ref_slice)) / (4*RADIUS*RADIUS)\n",
    "            errs[i][j] = err\n",
    "\n",
    "    # Resize the heatmap to match the original image\n",
    "    errs = skimage.transform.resize(errs, (ref_img.shape[0] - RADIUS * 2, ref_img.shape[1] - RADIUS * 2)).copy()\n",
    "    errs_new = np.ones(ref_img.shape[0:2]) * errs.mean()\n",
    "    errs_new[RADIUS:-RADIUS,RADIUS:-RADIUS] = errs\n",
    "    errs = errs_new\n",
    "\n",
    "    ax.imshow(errs, cmap=\"afmhot\", interpolation='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ref_img();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulating the onboard camera\n",
    "In the real world, a drone will use its onboard camera to capture an image of the ground directly beneath it. The area of terrain captured will depend on the focal length of the camera, which is known beforehand, and the altitude of the drone, which can be measured in real time. Thus, the image captured will be a rectangle — we chose to model it as a square for simplicity — with the drone's actual location at the center. \n",
    "\n",
    "This definition allows a straightforward simulation of the drone's onboard camera. \"Taking a picture\" in the simulation just means taking a slice of the reference image of a given size with the drone's true position — which is known exactly in the simulation — at the center. This of course does not account for changes in lighting, shadows or other movable objects, but we found we achieved promising results that validated our MVP with just this simple model. \n",
    "\n",
    "A function to simulate taking a picture from the drone's camera could look something like this (see [library module](TODO) for more detail):\n",
    "```python\n",
    "def take_picture(img, pos, radius):\n",
    "    return img[pos.x - radius : pos.x + radius, pos.y - radius : pos.y + radius]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = np.array((850,1500))\n",
    "\n",
    "ax = plt.subplot(1,2,1)\n",
    "plot_ref_img(ax, pos)\n",
    "\n",
    "img = take_picture(pos)\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(img)\n",
    "plt.title(\"View from Onboard Camera\")\n",
    "plt.axis('off');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing the captured image with the database\n",
    "To determine if the drone is in a particular location, we compare the image taken by the drone's camera with identically sized slices from the reference image. When the two images match, we know that the drone must be at the exact center of the matching slice. We define a straightforward algorithm for matching image slices: compare each pixel value for value and take the mean squared error, i.e.\n",
    "```python\n",
    "error = np.sum(np.square(test_slice - ref_slice)) / num_pixels\n",
    "```\n",
    "where test_slice is the image captured from the camera and ref_slice is a slice from the database.\n",
    "\n",
    "The graphic below demonstrates the result of searching across a horizontal line spanning the width of the image. The red line plots the result of the error function above as the test_slice slides across the line. As you can see, the minimum error occurs when the center of the test slice is at the actual position of the drone, indicating a match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.subplot(1,1,1)\n",
    "plot_ref_img(ax, pos, crop_height=.5)\n",
    "plot_line_error(pos, ax)\n",
    "# TODO deal with lengend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Searching the database for the captured image\n",
    "\n",
    "We can replicate this process by searching the entire map for a matching slice. The below graphic uses the same technique as above but we use a heatmap to plot the error. As can be seen, the point corresponding to the smallest error value is exactly the true position of the drone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,15))\n",
    "ax = plt.subplot(1,3,1)\n",
    "plot_ref_img(ax, pos)\n",
    "\n",
    "ax = plt.subplot(1,3,2)\n",
    "plot_ref_img(ax, pos)\n",
    "plot_heat_map(pos, ax)\n",
    "\n",
    "img = take_picture(pos)\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "plt.imshow(img)\n",
    "plt.title(\"Test Image\")\n",
    "plt.axis('off');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synthesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Informing the Search Algorithm\n",
    "1. Starting Point = from intertial nav\n",
    "1. Sprial Pattern\n",
    "1. Search Confidence\n",
    "### Combining Intertial + Image Recognition\n",
    "1. Putting the pieces together\n",
    "[Animation with both inertial + img rec]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ani = FlightAnimation()\n",
    "ani.add_target_start(path_true[0])\n",
    "ani.add_target_end(path_true[-1])\n",
    "ani.add_path(path_sim)\n",
    "ani.add_path(path_sim_plus)\n",
    "ani.start();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "[TODO: Make some over-generalizing statements]\n",
    "### Future Steps\n",
    "1. Increasing search radius dynamically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
